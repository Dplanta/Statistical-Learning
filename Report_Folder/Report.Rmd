---
title: "Titolo del progetto"
author: "Domenico Plantamura, Eduardo David Lotto, Manuel D'Alterio Grazioli, Gabriele Fugagnoli"
font: 12pt
output:
  html_document:
    toc: true
    # number_sections: true
  pdf_document:
    toc: true
    # number_sections: true
  word_document:
    toc: true
    # number_sections: true
---


```{r setup, include = FALSE}

knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

```


\

# Introduction

## Objective of the project

Our goal is to investigate whether the salaries earned by the NBA players during the 2023-2024 season are fair in proportion to their performance during the current year's Regular season. To analyse performance, we selected several statistics: from the most common such as points, rebounds, assists to advanced metrics like Usage, Player Impact Estimated and Winning Shares. 
The idea is to deep dive into the relationship between salaries and performance through different models in order to understand what kind of relationship there is and which model best fits the data. 
Finally, we will compare actual salaries with those predicted by our models to find out which players (according to the models) are the most overpaid or underpaid.


## Steps followed

To perform our analysis we followed these steps:

1.  Data collection;

2.  Data exploration;

3.  Analysis;

4.  Interpretation.

We now explain in depth each step.

\

## Data collection

We performed a web scraping operation from the [Official NBA Stats](https://www.nba.com/stats) website, from which we collected most of the stats. Additionally, we downloaded data about the salaries from [Hoopshype](https://hoopshype.com/) and other stats of interest from [Basketball reference](https://www.basketball-reference.com). 
All data concerns the 2023-2024 NBA Regular Season. 

### Why consider only Regular Season data?

Considering only data about Regular Season without considering players performance during playoffs limits a bit the potential of our analysis. On one hand, it's reasonable to infer that player performance during playoffs should have an important weight in determining his salary. On the other hand, considering playoffs in the analysis carries different issues.

There are teams (and consequently players) that go further than others: 14 out of 30 teams can't qualify for the playoffs. For the teams which qualify, playoff stats are calculated on a number of games that could differ greatly between different teams (e.g. if a team loses in the first round, it plays from 4 to 7 games. If a team reaches the finals, it plays from 16 to 28 games). During Regular Season every team plays a fixed number of games, 82.

Additionally, coaches usually rotate players at their disposal in a different way during playoffs: for instance, during regular season approximately 10-12 players for each team take part in the game; during playoffs it is not uncommon to observe only 7-8 players that come into play for each team. Furthermore, usually in a playoff game the best players are more involved compared to Regular season games. It means that, first of all, they play several more minutes. Moreover, they have the ball in their hands for a lot of time and consequently their stats grow a lot; hence, it could happen that few players record a large part of the entire team's statistics. Considering this, including playoffs data in the analysis could lead to an overestimation of performance of 2-3 players and to an underestimation of the performance of the rest of the team.

All in all, it is undeniable that playoffs are a fundamental part of the season. It is also obvious that if a player has more responsibilities in that phase he probably deserves a higher salary. But we think that for the purposes of our analysis, the addition of statistics collected on a small sample of matches, different for practically every team, with highly polarized data between the various players may lead to biases if not handled properly.

We think that considering only the regular season, although leading to a limited analysis, may be sufficient to grasp the main relationships between salaries and performance.


### Glossary

-	**PLAYER NAME**: name of a player;
-	**SALARY**: salary earned by a player for 2023-2024 season (collected from [Hoopshype](https://hoopshype.com/));
-	**AGE**: age of a player;
-	**POS**: “Position”, the playing position of a player.

#### Traditional stats (collected from the [NBA](https://www.nba.com/?47) website)

-	**GP**: “Games played”, the number of games played by a player during the 2023-2024 regular season;
-	**FG_PCT**: “Field Goal Percentage”, the percentage of field goal attempts that a player makes. Formula: (FGM)/(FGA);
-	**FG3_PCT**: “3 Points “Field Goal Percentage”, the percentage of 3pt field goal attempts that a player makes;
-	**FT_PCT**: “Free throws Percentage”, the percentage of free throws attempts that a player makes;
-	**OREB**: “Offensive Rebounds”, the number of rebounds a player or team has collected while they were on offense;
-	**DREB**: “Defensive Rebounds”, the number of rebounds a player or team has collected while they were on defense;
-	**REB**: “Rebounds”, a rebound occurs when a player recovers the ball after a missed shot. This statistic is the number of total rebounds a player has collected on either offense or defense;
-	**AST**: “Assists”, the number of assists (passes that lead directly to a made basket) by a player;
-	**TOV**: “Turnovers”, a turnover occurs when a player on offense loses the ball to the defense;
-	**STL**: “Steals”, number of times a defensive player takes the ball from a player on offense, causing a turnover;
-	**BLK**: “Blocks”, a block occurs when an offensive player attempts a shot, and the defense player tips the ball, blocking their chance to score;
-	**BLKA**: “Blocks Against”, The number of shots attempted by a player or team that are blocked by a defender
-	**PF**: “Personal fouls”, the number of personal fouls a player or team committed;
-	**PFD**: “Personal fouls drawn”, the number of personal fouls that are drawn by a player or team;
-	**PTS**: “Points”, the number of points scored by a player;
-	**MIN**: “Minutes played”, number of minutes played by a player during the 2023-2024 Regular season;
-	**MIN_G**: “Minutes played per game”.

#### Advanced stats (collected from the [NBA](https://www.nba.com/?47) website)

-	**OFF_RATING**: “Offensive Rating”, measures a team's points points scored per 100 possessions while a player is on the court. Formula: 100*((Points)/(POSS);
-	**DEF_RATING**: “Defensive Rating”, the number of points per 100 possessions that the team allows while a player is on the court. Formula: 100*((Opp Points)/(Opp POSS));
-	**NET_RATING**: “Net Rating”, Measures a team's point differential per 100 possessions while a player is on the court. Formula: OFFRTG - DEFRTG;
-	**AST_TO**: “Assist to Turnover Ratio”, the number of assists for a player compared to the number of turnovers committed;
-	**TS_PCT**: “True Shooting Percentage”, a shooting percentage that factors in the value of three-point field goals and free throws in addition to conventional two-point field goals. Formula: Points/ [2*(Field Goals Attempted+0.44*Free Throws Attempted)];
-	**USG_PCT**: “Usage Percentage”, the percentage of team plays used by a player when they are on the floor. Formula: (FGA + Possession Ending FTA + TO) / POSS;
-	**PIE**: “Player Impact Estimate”, measures a player's overall statistical contribution against the total statistics in games they play in. PIE yields results which are comparable to other advanced statistics (e.g. PER) using a simple formula. Formula: (PTS + FGM + FTM - FGA - FTA + DREB + (.5 * OREB) + AST + STL + (.5 * BLK) - PF - TO) / (GmPTS + GmFGM + GmFTM - GmFGA - GmFTA + GmDREB + (.5 * GmOREB) + GmAST + GmSTL + (.5 * GmBLK) - GmPF - GmTO).

The stats below are collected from [Basketball Reference](https://www.basketball-reference.com):

-	**WS**: “Win Shares”, attempts to divvy up credit for team success to the individuals on the team. It is calculated using player, team and league-wide statistics and the sum of player win shares on a given team will be roughly equal to that team’s win total for the season (more details on the [Basketball Reference page](https://www.basketball-reference.com/about/ws.html));
-	**BPM**: “Box Plus/Minus”, a box score estimate of the points per 100 possessions that a player contributed above a league-average player, translated to an average team;
-	**VORP**: “Value Over Replacement Player”, a box score estimate of the points per 100 TEAM possessions that a player contributed above a replacement-level (-2.0) player, translated to an average team and prorated to an 82-game season. Multiply by 2.70 to convert to wins over replacement.

BPM and VORP are calculated per 100 possessions; MIN and WS are calculated over the whole regular season, MIN_G is calculated per game. The other stats are considered per 48 minutes.


### Why statistics per 48 minutes?

Considering most statistics projected over 48 minutes avoids overestimating performance for players who play, on average, more minutes in a game. In this way we think that the contribution of each player is fairly evaluated and not distorted by the minutes played.

```{r useful-code1, include=FALSE}

library(readxl)
data_salary <- read_excel("./data/NBA_players_salaries_HH.xlsx")
data_traditional_per48 <- read.csv("./data/RS_traditional_per48.csv")
data_traditional_tot <- read.csv("./data/RS_traditional_TOTALS.csv")
data_advanced <- read.csv("./data/RS_advanced_per48.csv")
data_miscellaneous <- read.csv("./data/RS_miscellaneous_per48.csv")
data_vorp <- read_excel("./data/vorp.xlsx")
MIN_G <- data_traditional_tot$MIN/data_traditional_tot$GP
data_traditional_tot <- cbind(data_traditional_tot, MIN_G)
data_salary <- data_salary[, c(2, 3)]
data_traditional_per48 <- data_traditional_per48[, c(3, 7, 8, 15, 18, 21:32)]
data_traditional_tot <- data_traditional_tot[, c(3, 12, 68)]
data_advanced <- data_advanced[, c(3, 14, 17, 20, 23, 31, 32, 38)]
data_miscellaneous <- data_miscellaneous[, c(3, 24)]
data_vorp <- data_vorp[, c(2, 3, 23, 31, 32)]
names(data_salary)[names(data_salary) == "Player"] <- "PLAYER_NAME"
```

## Data integration and cleaning

Once we had obtained the tables of interest, we selected from each table the statistics useful for analysis (those given in the glossary) and then merged the slices of the various datasets, removing all the players who played less than 480 minutes during the entire regular season.

```{r datasets-merging}
data_trad_tot <- data_traditional_tot[data_traditional_tot$MIN > 480, ]

data_st <- merge(data_salary, data_traditional_per48, by = "PLAYER_NAME", all = TRUE)
data_ast <- merge(data_st, data_advanced, by = "PLAYER_NAME", all = TRUE)
data_mast <- merge(data_ast, data_miscellaneous, by = "PLAYER_NAME", all = TRUE)
data_mastt <- merge(data_mast, data_trad_tot, by = "PLAYER_NAME", all = TRUE)
final_dataset <- merge(data_mastt, data_vorp, by = "PLAYER_NAME", all = TRUE)
```

The reason why we selected players with at least 480 minutes played is that we wanted to avoid considering stats taken on a too small amount of minutes.
After these operation, the final dataset consists of 360 rows and 31 columns.

At this stage, we cleaned the data following these other steps:

- NA removal;
- Matching players' names;
- Transforming the Salary column into a numeric one;
- Putting the players' name as row names for the dataset and thus removing the PLAYER_NAME column.

```{r useful-code2, include=FALSE}
final_dataset <- final_dataset[!is.na(final_dataset$AGE), ]
final_dataset <- final_dataset[!is.na(final_dataset$MIN), ]
final_dataset <- final_dataset[!is.na(final_dataset$VORP), ]
final_dataset <- final_dataset[, -17]
colnames(final_dataset)[colnames(final_dataset) == 'PFD.y'] <- 'PFD'
colnames(final_dataset)[colnames(final_dataset) == '2023/24'] <- 'Salary'
final_dataset$Salary <- as.numeric(gsub("[\\$\\,]", "", final_dataset$Salary))
rownames(final_dataset) <- final_dataset$PLAYER_NAME
final_dataset <- final_dataset[, -1]
attach(final_dataset)
```


## Data exploration

Before studying the data with formal models, we got an overview through an exploratory data analysis. For the first part of our analysis we used only numeric variables, so the categorical parameter 'Pos', which you can see on the table below, was removed from the dataset at this stage.  

```{r example-of-dataset, echo=FALSE, warning=FALSE}
library(knitr)
kable(final_dataset[1:5, 1 :10], format="simple", align = "cccccccccc")
kable(final_dataset[1:5, 11:20], format="simple", align = "cccccccccc")
kable(final_dataset[1:5, 21:30], format="simple", align = "cccccccccc")
```

Firstly, an analysis of the variable Salary that will be the dependent variable in the models.

```{r variable-salary, echo=FALSE, message=FALSE, warning=FALSE}
numeric_cols <- sapply(final_dataset, is.numeric)
fd_numeric <- final_dataset[, numeric_cols]
par(mfrow = c(2, 2))
boxplot(Salary, main="Boxplot of the salary")
hist(Salary, main="Histogram of the salary")
boxplot(log(Salary),  main="Logarithmic salary", ylab="Salary in millions")
hist(log(Salary),  main="Logarithmic salary", xlab="Salary in millions")
par(mfrow = c(1, 1))
```

The boxplot shows that the salary distribution is right skewed, with some outliers in the right side. We expected this kind of distribution, the outliers are the players earning the highest salaries. The histogram also highlights the right skewed distribution. It can be seen that Salary's log transformation reduces the skewness and makes the distribution of the variable closer to normal. 

```{r boxplots-independent-variables, include=FALSE}
boxplot(AGE, main="AGE", names=c("AGE"), show.names=TRUE, ylab="years")
boxplot(GP, main="Games played", names=c("GP"), show.names=TRUE, ylab="number of GP")
boxplot(MIN, main="MiN played per season", names=c("MIN"), show.names=TRUE, ylab="minutes played")
boxplot(MIN_G, PTS, main="MIN played and PTS scored per game", names=c("MIN_G", "PTS"), ylab="units number")
boxplot(OREB, DREB, REB, AST, main="OREB, DREB, REB, AST per game", names=c("OREB", "DREB", "REB", "AST"), ylab="units number")
boxplot(TOV, STL, BLK, BLKA, PF, PFD, main="TOV, STL, BLK, BLKA, PF, PFD per game", names=c("TOV", "STL", "BLK", "BLKA", "PF", "PFD"), ylab="units number")
boxplot(FG_PCT, FG3_PCT, FT_PCT, TS_PCT, main="FG_PCT, FG3_PCT, FT_PCT, TS_PCT", names=c("FG_PCT", "FG3_PCT", "FT_PCT", "TS_PCT"), ylab="percentage")
boxplot(OFF_RATING, DEF_RATING, main="OFF_RATING, DEF_RATING", names=c("OFF_RATING", "DEF_RATING"), ylab="rating")
boxplot(NET_RATING, main="NET_RATING", names=c("NET_RATING"), show.names=TRUE, ylab="rating")
boxplot(AST_TO, main="AST_TO", names=c("AST_TO"), show.names=TRUE, ylab="units number")
boxplot(PIE, USG_PCT, main="PIE, USG_PCT", names=c("PIE", "USG_PCT"), ylab="percentage")
boxplot(WS, BPM, VORP, main="WS, BPM, VORP", names=c("WS", "BPM", "VORP"), ylab="score")
```

In order to study correlations between the predictors of the model, we used the corrplot function.

```{r corrplot-independent-variables, message=FALSE, warning=FALSE}
library(corrplot)
corrplot(cor(fd_numeric), method = 'color')
```

Different correlations between the variables emerge from the corrplot. 
With regard to the variable Salary, it is interesting to notice that Salary is positively correlated with PTS and advanced stats like USG_PCT, BPM and VORP: all of these variables are related to players' shots and point contribution.
For what concerns the other variables, there are some obvious correlations: for instance, between variables MIN (total minutes played during the regular season) and MIN_G (minutes played per game) and between variables REB, OREB and DREB (all related to rebounds, with the relation REB = OREB + DREB). Additionally, we expected the positive correlation between BPM and VORP because are both related to players point estimation. 
A strong positive correlation emerges between PTS and USG_PCT. The usage percentage is "The percentage of team plays used by a player when they are on the floor. Formula: (FGA + Possession Ending FTA + TO) / POSS". Thus, players with a high USG_PCT often make the last play in an offensive possession (a shot, a free throw or a turnover): it is straightforward that if a player often ends the offensive possession of his team, he has more opportunities to score points.
For what concerns the negative correlations, the most interesting are the ones between rebounds variables (OREB, DREB, REB), FT_PCT and FG3_PCT. Players that grab a lot of rebounds are usually the tallest ones and these players are not great free throws shooters or 3 point shooters (on average).


## Models

We started creating a linear regression model in order to predict salaries.

```{r linear-regression-model, echo=FALSE}

lm.mod <- lm(Salary~+., data=fd_numeric)
summary(lm.mod)

# Residual analysis
par(mfrow = c(2, 2))
plot(lm.mod)

# MSE
lm.mod.pred <- predict(lm.mod)
y <- fd_numeric$Salary
test.lm.mod <- mean((lm.mod.pred-y)^2)
test.lm.mod

```

The complete model has a good adjusted R-squared of 0.68 and a MSE of 4.14e+13.
It emerges that many variables are not significative in determining the response. 
Through the residual analysis it is noticeable that the relationship between fitted values and residuals is not exactly linear (1st graph). Additionally, in the third graph the points are not are included in a band of constant amplitude parallel to the x-axis, hence the omoschedasticity assumption can be doubted. 


```{r logarithmic-transformation, echo=FALSE}
lm.log <- lm(log(Salary)~+., data=fd_numeric)
summary(lm.log)
par(mfrow = c(2, 2))
plot(lm.log)

lm.log.pred <- predict(lm.log)
test.lm.log <- mean((exp(lm.log.pred)-y)^2)
test.lm.log
```

With a logarithmic transformation of the dependent variable, the model shows a slightly lower adjusted R-squared (0.62 vs the previous 0.68) and a slightly higher MSE (4.70e+13 vs the previous 4.14e+13). 
Applying a logarithmic transformation to the dependent variable, the first graph shows a more linear relationship and the third graph allows to infer a more constant variance in the error terms.
In both models many variables are not significative in determining the response: for this reason, to avoid a model that is unnecessary complex, we performed a variable selection. A logarithmic transformation of the dependent variable Salary will be applied because, although it slightly worsens the performance of the model, it makes the salaries distribution closer to normal, it improves the linearity of the model and it reduces residuals eteroschedasticity.


### Variable selection

We selected a subset of relevant features starting from the predictors used in the complete model in order to have a simpler model that is easier to interpret, without redundant variables and less prone to overfitting.
To do so, we used The regsubsets function which performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. We set the function to return results up to the best 28-variables model.

```{r exhaustive-subset-selection, echo=FALSE, include=FALSE}
library(leaps)

regfit.full <- regsubsets(log(Salary)~., data=fd_numeric, nvmax=(ncol(fd_numeric)-1))
reg.summary <- summary(regfit.full)
reg.summary$outmat
reg.summary$which
reg.summary$rsq
```

To find the best balance between model simplicity and precision, we evaluated the number of parameters to be included in the model through Mallow's Cp, BIC and Adjusted R-squared.

```{r parameters-selection, echo=FALSE}
par(mfrow = c(2, 2))

plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")

plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted Rsq",type="l")
i <- which.max(reg.summary$adjr2)
points(i,reg.summary$adjr2[i], col="red",cex=2,pch=20)
text(i,reg.summary$adjr2[i], i, pos=1)

plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
i <- which.min(reg.summary$cp)
points(i,reg.summary$cp[i],col="red",cex=2,pch=20)
text(i,reg.summary$cp[i], i, pos=3)

plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
i <- which.min(reg.summary$bic)
points(i,reg.summary$bic[i],col="red",cex=2,pch=20)
text(i,reg.summary$bic[i], i, pos=3)
par(mfrow = c(1,1))
```

Considering Mallow's Cp, the best number of parameters for our model is 12. We obtained the list of parameters from the regsubset function to get the best model with 12 parameters. 

```{r best_model_12_parameters, echo=FALSE}
covariates = 12

selected.model <- reg.summary$which[covariates,]
selected.parameters <- names(selected.model[selected.model])[-1]
selected.formula <- as.formula(paste("log(Salary) ~", paste(selected.parameters, collapse = " + ")))

lm.exhaustive <- lm(selected.formula, data=fd_numeric)
summary(lm.exhaustive)
```
The reducted model shows a higher adjusted R-squared, 0.63, compared to the complete logarithmic model (0.62). It means that, despite the lower number of variables, this model fits better the data. 
Different variables are strongly significant:

- AGE: the positive coefficient associate to the variable show that older players earn, on average, more than younger ones.
This makes sense because the youngest players in the league, rookies(first year in NBA) and sophmores(second year in NBA),
usually earn less in the first years due to particular specifications in their contracts.

- PTS: quite straightforward: positive coefficient means that the players who scores more points, on average, have higher         salaries.

- TS_PCT: for what concerns true shooting percentage, the situation is peculiar. TS_PCT weights a player's shooting percentages based on the shot type (3-pointer, 2 pointer or free throw). The negative coefficient seems counterintuitive, it means that a better TS_PCT reflects, on average, a lower salary. A possible explanation is that this metric is high for two players categories: The first category is composed by big men which take most of their shots near the basket, thus high percentage shots. The second category is composed by 3-point shooting specialists, because in the metric the weight for a 3 point shoot is higher. The mentioned players are really important into a team, but we can say that they often have a limited role: the former have to score mostly near the basket, the latter from behind the 3-point line. Consequently, it makes sense a lower salary for players with a limited role. Additionally, shooting percentages are also high for players that shoot only few shots in a game; it is reasonable that scoring only few shots it's not enough to earn high salaries. 

- MIN_G: players that play on average more minutes in a game earn, on average, a higher salary.


The variable FG_PCT is less significative than TS_PCT, but the coefficient here is positive. Both the stats measure shooting percentages, but FG_PCT does not weight shots and does not consider free throws. In this way, the previous mentioned effect on 3 point shooting specialists reduces. It is possible to infer that FG_PCT represents better, within this model, the positive impact of good shooting percentages on wages.

With a level of significance between 0.01 and 0.05 the variables OFF_RATING, DEF_RATING, NET_RATING, PIE and WS. The positive sign of OFF_RATING and WS coefficients and the negative sign of DEF_RATING coefficient are in line with what we expected. OFF_RATING (DEF_RATING) represents the points scored (conceded) by the team when the player is playing, WS measures the player contribution to the team wins. We didn't expected a negative signs for NET_RATING (OFF_RATING - DEF_RATING) and PIE, that measures the player impact in the game. 
For what concerns PIE, the negative sign has different possible explanations: projecting PIE per 48 minutes, it is inflated for players who have a high impact on the game but only for few minutes; It considers a lot of stats, even stats that seem to be not significative in determining salary; PIE difference between high salary players and low salary ones is not proportional to the differences in salaries; it is always difficult consider defensive contribution with this kind of metrics and it is reasonable to think that defensive contribution plays an important role in determining a players salary; PIE does not consider aspects like leadership and IQ that, like defensive contribution, will certainly have an impact on the salaries.
Anyway, beyond all the possible explanations, these unexpected negative signs likely depend from other variables not included in the model.

#### Correlation between dependent variables

```{r correlation-independent-variables, echo=FALSE}

# correlation between dependent variables
par(mfrow=c(1,1))
corrplot(cor(fd_numeric[c(selected.parameters)]), method = 'number')

```

There are, also in this case, different correlations between the dependent variables.

#### Residual analysis


```{r residual-analysis, echo=FALSE}

# residual analysis
par(mfrow=c(2,2))
plot(lm.exhaustive)

```

From the plots the assumptions of the linear model seem to be fulfilled. It can be seen that some players are outliers in each graph: these players probably have special contracts (two-way contracts). This means that they usually play in the team's second team (in a so called development league) and occasionally in the first team, so they have really low salaries compared to the league average.


#### MSE

```{r model-performance, echo=FALSE}

y <- fd_numeric$Salary

# MSE
lmex.final.pred <- predict(lm.exhaustive)
test.mse.lmex <- mean((exp(lmex.final.pred)-y)^2)
test.mse.lmex

```

The Mean Squared Error of the reduced model is really close to the complete model one (with the logarithmic transformation of the dependent variable), 4.77e+13 against 4.70e+13. Considering that the complete model has 28 variables and the reduced one 12, the latter model represents quite an improvement.

#### Real salaries vs salaries prediction

```{r overpaid-underpaid_check, echo=FALSE}

### 10 most overpaid and 10 most underpaid players table ###
res <- y - exp(lmex.final.pred)

overpaid_indices <- order(res, decreasing=TRUE)[1:10]
underpaid_indices <- order(res, decreasing=FALSE)[1:10]

over_diff <- res[overpaid_indices]
under_diff <- res[underpaid_indices]

over_pred <- exp(lmex.final.pred)[overpaid_indices]
under_pred <- exp(lmex.final.pred)[underpaid_indices]

## actual salary and player names
fd_over <- final_dataset[overpaid_indices, ][c('Salary')]
fd_under <- final_dataset[underpaid_indices, ][c('Salary')]

overpaid_tab_ex <- cbind(fd_over, over_pred, over_diff)
colnames(overpaid_tab_ex) <- c("Salary", "Predicted salary", "Difference")
underpaid_tab_ex <- cbind(fd_under, under_pred, -under_diff)
colnames(underpaid_tab_ex) <- c("Salary", "Predicted salary", "Difference")
overpaid_tab_ex
underpaid_tab_ex

```

Here we have a comparison between real salaries and predicted ones. The tables contain, respectively, the most overpaid players and the most underpaid players according to the model.

*MOST OVERPAID PLAYERS*

The most overpaid player results to be Bradley Beal. After some brilliant seasons with Washington Wizards in which he was the league top scorer, he signed in 2022 a maximum contract (251 million $ in 5-years). In Washington he was the best player by far, his statlines in the past years justify the huge contract. In 23-24 he was traded to Phoenix (keeping the same contract) to play with Durant and Booker (two superstars) in a team that was, on the paper, a contender for the title. Beal, being no longer the first offensive option, had a quite different statline compared to the previous years. Additionally, the whole Phoenix Suns team disappointed expectations. These facts are enough to explain that Beal 23-24 performance is not in line with his salary.
Darius Garland signed a big contract (near to the maximum) starting from 23-24 season. After showing superstar potential in 22-23, Cleveland Cavaliers renewed his contract with an important salary increase but Garland's performance decreased in 23-24. He is only 24, the team bet heavily on him taking a weighted risk in order to keep with them a high potential player. This bet didn't paid in 23-24 season.
Trae Young and Zach Lavine have superstar contracts respectively in Atlanta and Chicago, but they are not carrying their teams as expected. Both players could be traded during this summer.
Zion Williamson and Michael Porter jr (especially the first one) are young players that in their still short careers have not shown their full potential due to injuries. Their contracts, let's say, consider their potential performance at the top of their form.
Jordan Poole had an exploit in the previous seasons playing with a top team, Golden State Warriors, that somehow justifies his salary. He seemed to be ready to carry a team on his own, he was traded to Washington but his first season was a failure. 

*MOST UNDERPAID PLAYERS*

Lebron James and Kevin Durant are two of the best players in the league for many years now. Even though, according to our model they should earn much more than the maximum wage. For sure their careers and their performance motivate a high salary, but equally surely they are not underpaid. We think that this overestimation depends in part on the fact that the variable AGE in the model is strongly significant, Lebron James is 39 and Kevin Durant is 35. The same reasoning could apply to Kyrie Irving (32) and especially Demar Derozan (34).
For what concerns Nikola Vucevic, his stats are always more than respectable. His salary is lower than the expected probably because he seems to lack characteristics not included in the model or generally difficult to quantify such as defense, leadership and consistency at key moments of the season.
Jalen Brunson has shown this year that he is one of the best players in the NBA after being somewhat underrated in the years past. We expected the difference between his predicted and actual salary. Very similar the situation of Tyrese Maxey, in the last year of his rookie contract. He has shown by his performances that he is worth much more than his salary says. 
Russell Westbrook is in the waning phase of his career. On the expiry of his last superstar contract, no team in the league offered him a comparable salary (he earned 47 millions in 2022). Consequently, he accepted a 3.8 millions salary (veteran minimum contract) to play with Los Angeles Clippers. For sure he is no longer a player worth 47 millions, but he is not worth 3.8 millions either. Our model interprets pretty well the situation, stating that Westbrook should earn a 18.3 millions salary: not a superstar one, but not a minimum wage either. 



Due to the presence of correlations between the independent variables, we decided to implement models that perform well when the variables are collinear such as Ridge regression and Lasso regression. In the next paragraphs we want to see if the performance of these models is better than that of the models seen so far.



### Ridge regression

The subset selection methods use least squares to fit a linear model with a subset of the predictors. Instead of controlling model complexity by setting a subset of coefficients beta j to be zero, ridge regression fits a model with all p predictors shrinking the coefficient estimates towards zero. Ridge regression uses quadratic shrinking. 

```{r ridge-starting, include=FALSE}

library(glmnet)

# linear model
lm.mod <- lm(Salary~+., data=fd_numeric)
summary(lm.mod)

# design matrix not considering the intercept
X <- model.matrix(Salary~., data=fd_numeric)
X <- X[,-1]

# vector of responses
y <- fd_numeric$Salary

```

Lambda is a tuning parameter. In order to determine a good value of lambda, we used cross-validation.  

```{r lambda-selection, echo=FALSE}
### Cross validation to select the best lambda ###

# select n/2 observations for training set
n <- nrow(X)

set.seed(1)
train <- sample(1:n, 180)
test  <- setdiff(1:n, train)

cv.out <- cv.glmnet(X[train, ], y[train], alpha = 0, nfold=10)

# This plots the cross-validation curve (red dotted line) along with upper and
# lower standard deviation curves along the lambda sequence (error bars).
# Two special values along the lambda sequence are indicated by the vertical
# dotted lines. lambda.min is the value of lambda that gives minimum mean
# cross-validated error, while lambda.1se is the value of lambda that gives
# the most regularized model such that the cross-validated error is within one
# standard error of the minimum.
plot(cv.out)

## selecting the lambda that minimizes test MSE
best_lambda <- cv.out$lambda.min
best_lambda
 
# estimated test MSE with bestlambda value
ridge.mod <- glmnet(X[train, ], y[train], alpha=0)
ridge.pred <- predict(ridge.mod, s=best_lambda, newx=X[test,])
test.mse.ridge <- mean((ridge.pred-y[test])^2)
test.mse.ridge
```
In the plot above the red dotted line represents the cross-validation curve (red dotted line) along with upper and lower standard deviation curves along the lambda sequence (error bars). We chose the value of lambda (755675.4) that gives minimum mean cross-validated error.
The mean squared error on the test set is 5.45e+13.

```{r model-with-best-lambda, echo=FALSE}

## final model with best lambda on all data
ridge.final <- glmnet(X, y, alpha = 0)
coef(ridge.final, s=best_lambda)

# Trace plot to visualize how the coefficient estimates changed as a result of increasing lambda
plot(ridge.final, xvar="lambda", label=TRUE)
abline(v=log(best_lambda), lty=3, lwd=2)


```

The final model was fitted with the best lambda on all data. The trace plot shows how the coefficients change if lambda increases.

```{r evaluating-performances, echo=FALSE}

# R2

#use fitted best model to make predictions
y_predicted <- predict(ridge.final, s = best_lambda, newx = X)

#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)

#find R-Squared
R2 <- 1 - sse/sst
R2

## R2 better than regression with exhaustive subset search

# final MSE
ridge.final.pred <- predict(ridge.final, s=best_lambda, X)
mse.ridge <- mean((ridge.final.pred-y)^2)
mse.ridge

## mse better than regression with exhaustive subset search

```
Once fitted the model on all data with the best lambda, we evaluated the performance of the model. The 0.69 R-squared highlights a better fit compared to the previous model (0.64) obtained after a subset selection. Also the MSE improves, 4.34e+13 vs 4.77e+13.

The final step is the comparison between real salaries and predicted ones.

```{r final-comparison, echo=FALSE}

### 10 most overpaid and 10 most underpaid players table ###

diffs <- y - ridge.final.pred
diffs <- as.vector(diffs)

overpaid_indices <- order(diffs, decreasing=TRUE)[1:10]
underpaid_indices <- order(diffs, decreasing=FALSE)[1:10]

over_diff <- diffs[overpaid_indices]
under_diff <- diffs[underpaid_indices]

over_pred <- ridge.final.pred[overpaid_indices]
under_pred <- ridge.final.pred[underpaid_indices]

## actual salary and player names
fd_over <- final_dataset[overpaid_indices, ][c('Salary')]
fd_under <- final_dataset[underpaid_indices, ][c('Salary')]

## final table for ridge regression
overpaid_tab_ridge <- cbind(fd_over, over_pred, over_diff)
colnames(overpaid_tab_ridge) <- c("Salary", "Predicted salary", "Difference")
underpaid_tab_ridge <- cbind(fd_under, under_pred, -under_diff)
colnames(underpaid_tab_ridge) <- c("Salary", "Predicted salary", "Difference")

overpaid_tab_ridge 
underpaid_tab_ridge

```

*MOST OVERPAID PLAYERS*

Here there are a different similarities between the previous model: Beal, Pool, LaVine, Ayton, Garland and Porter Jr. still result in the most overpaid players. 
Klay Thompson, after being a key piece in the Golden State Warriors dinasty, suffered a serious injury few years ago. After that, he was no longer the same player and the salary was, let's say, no longer adequate to his performance. His contract with Golden State ended after the 23-24 season and he recently signed with Dallas Mavericks for 50 millions in 3 years, thus he will earn a salary closer to the predicted one.


