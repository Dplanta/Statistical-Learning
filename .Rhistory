# check the components of the reg.summary objects
names(reg.summary)
reg.summary$rsq
par(mfrow=c(2,2))
# residual sum of squares
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
# adjusted-R^2 with its largest value
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted Rsq",type="l")
i <- which.max(reg.summary$adjr2)
points(i,reg.summary$adjr2[i], col="red",cex=2,pch=20)
text(i,reg.summary$adjr2[i], i, pos=1)
# Mallow's Cp with its smallest value
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
i <- which.min(reg.summary$cp)#return the index of the minimum
points(i,reg.summary$cp[i],col="red",cex=2,pch=20)
text(i,reg.summary$cp[i], i, pos=3)
# BIC with its smallest value
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
i <- which.min(reg.summary$bic)
points(i,reg.summary$bic[i],col="red",cex=2,pch=20)
text(i,reg.summary$bic[i], i, pos=3)
par(mfrow=c(1,1))
plot(regfit.full, scale="r2")
plot(regfit.full, scale="adjr2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
# coefficients of the best model according to BIC
coef(regfit.full, 6)
# fit the selected model
best.bic <- lm(Salary~AtBat+Hits+Walks+CRBI+Division+PutOuts, data=Hitters)
summary(best.bic)
plot(best.bic, which = 1)
plot(best.bic, which = 2)
plot(best.bic, which = 3)
n <- nrow(Hitters)
th <- 3 *(6+1)/n
plot(best.bic, which = 5)
abline(v = th, lty=3)
regfit.full.log <- regsubsets(log(Salary)~., data=Hitters)
reg.summary.log <- summary(regfit.full.log)
reg.summary.log$bic
which.min(reg.summary.log$bic)
coef(regfit.full.log, 3)
best.bic.log <- lm(log(Salary)~Hits+Walks+Years, data=Hitters)
summary(best.bic.log)
plot(best.bic.log, which = 1)
plot(best.bic.log, which = 2)
plot(best.bic.log, which = 3)
plot(best.bic.log, which = 1)
plot(best.bic.log, which = 1)
plot(best.bic.log, which = 2)
plot(best.bic.log, which = 3)
# coefficients of the best model according to BIC
coef(regfit.full, 6)
# fit the selected model
best.bic <- lm(Salary~AtBat+Hits+Walks+CRBI+Division+PutOuts, data=Hitters)
summary(best.bic)
# check how r-squared changes
reg.summary$rsq
plot(best.bic, which = 1)
plot(best.bic, which = 2)
plot(best.bic, which = 3)
p.plus.1 <- length(coef(best.bic))
n <- nrow(Hitters)
th <- 3 *p.plus.1/n
plot(best.bic, which = 5)
abline(v = th, lty=3)
regfit.full.log <- regsubsets(log(Salary)~., data=Hitters, nvmax=19)
reg.summary.log <- summary(regfit.full.log)
reg.summary.log$bic
which.min(reg.summary.log$bic)
coef(regfit.full.log, 3)
best.bic.log <- lm(log(Salary)~Hits+Walks+Years, data=Hitters)
summary(best.bic.log)
# check how the r-squared changes
reg.summary.log$rsq
plot(best.bic.log, which = 1)
plot(best.bic.log, which = 1)#We observe an improved variance
plot(best.bic.log, which = 2)
plot(best.bic.log, which = 3)
p.plus.1 <- length(coef(best.bic.log))
th <- 3 *p.plus.1/n
plot(best.bic.log, which = 5)
abline(v = th, lty=3)
th <- 3 *(4+1)/n
best.bic.log.poly <- lm(log(Salary)~poly(Hits, 2)+Walks+poly(Years, 2), data=Hitters)
summary(best.bic.log.poly)
#obtained through trial and error:
best.bic.log.poly <- lm(log(Salary)~poly(Hits, 2)+Walks+poly(Years, 2), data=Hitters)
summary(best.bic.log.poly)
plot(best.bic.log.poly, which = 1)
plot(best.bic.log.poly, which = 2)
plot(best.bic.log.poly, which = 2)
plot(best.bic.log.poly, which = 3)
p.plus.1 <- length(coef(best.bic.log.poly))
th <- 3 *p.plus.1/n
plot(best.bic.log.poly, which = 5)
abline(v = th, lty=3)
X <- model.matrix(Salary~., data=Hitters)
out <- lm(Salary~., data=Hitters)
beta  <- coef(out)
y.hat <- X%*%beta
max(abs(y.hat-fitted(out)))
n/2
set.seed(111)
train <- sample(1:263,  size=132)
test  <- setdiff(1:263, train)
regfit.best <- regsubsets(Salary~., data=Hitters[train,], nvmax=19)
test.mat <- model.matrix(Salary~., data=Hitters[test,])
beta <- coef(regfit.best, id=3)
pred <- test.mat[, names(beta)]%*%beta
mean((Hitters$Salary[test]-pred)^2)
val.errors <- rep(NA,19)
for(i in 1:19){
beta <- coef(regfit.best, id=i)
pred <- test.mat[,names(beta)]%*%beta
val.errors[i] <- mean((Hitters$Salary[test]-pred)^2)
}
val.errors
plot(val.errors,type='b')
i <- which.min(val.errors)
points(i,val.errors[i], col="red",cex=2,pch=20)
#set.seed(111)
#set.seed(222)
set.seed(123)
train <- sample(1:263,  size=132)
test  <- setdiff(1:263, train)
regfit.best <- regsubsets(Salary~., data=Hitters[train,], nvmax=19)
test.mat <- model.matrix(Salary~., data=Hitters[test,])
beta <- coef(regfit.best, id=3)
pred <- test.mat[, names(beta)]%*%beta
mean((Hitters$Salary[test]-pred)^2)
val.errors <- rep(NA,19)
for(i in 1:19){
beta <- coef(regfit.best, id=i)
pred <- test.mat[,names(beta)]%*%beta
val.errors[i] <- mean((Hitters$Salary[test]-pred)^2)
}
val.errors
plot(val.errors,type='b')
i <- which.min(val.errors)
points(i,val.errors[i], col="red",cex=2,pch=20)
# set a value for k
k=10
set.seed(111)
folds <- sample(1:k, nrow(Hitters), replace=TRUE)
folds[1:20]
table(folds)
cv.errors <- matrix(NA, k, 19)
colnames(cv.errors) <- 1:19
# apply the validation set approach k times,
# one for each fold
#
for(j in 1:k){
best.fit <- regsubsets(Salary~., data=Hitters[folds!=j,], nvmax=19)
test.mat <- model.matrix(Salary~., data=Hitters[folds==j,])
for(i in 1:19){
beta <- coef(best.fit, id=i)
pred <- test.mat[,names(beta)]%*%beta
cv.errors[j,i] <- mean((Hitters$Salary[folds==j]-pred)^2)
}
}
# compute the mean of CV errors across the k folds
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
# apply the validation set approach k times,
# one for each fold
#
for(j in 1:k){
best.fit <- regsubsets(Salary~., data=Hitters[folds!=j,], nvmax=19)
test.mat <- model.matrix(Salary~., data=Hitters[folds==j,])
for(i in 1:19){
beta <- coef(best.fit, id=i)
pred <- test.mat[,names(beta)]%*%beta
cv.errors[j,i] <- mean((Hitters$Salary[folds==j]-pred)^2)
}
}
# compute the mean of CV errors across the k folds
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
# check the (low) variability of this plot for different
# choices of random seeds
#
plot(mean.cv.errors, type='b')
i <- which.min(mean.cv.errors)
points(i, mean.cv.errors[i], col="red",cex=2,pch=20)
# fit the selected model on the full dataset
reg.best <- regsubsets(Salary~.,data=Hitters, nvmax=19)
coef(reg.best, i)
# forward inclusion
#
regfit.fwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
plot(regfit.fwd, scale="bic")
fwd.summary <- summary(regfit.fwd)
plot(fwd.summary$bic, xlab="Number of Variables", ylab="BIC",type='l')
i <- which.min(fwd.summary$bic)
i
points(i,reg.summary$bic[i],col="red",cex=2,pch=20)
# backward elimination
#
regfit.bwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="backward")
plot(regfit.bwd, scale="bic")
bwd.summary <- summary(regfit.bwd)
plot(bwd.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
i <- which.min(bwd.summary$bic)
i
points(i,bwd.summary$bic[i],col="red",cex=2,pch=20)
coef(regfit.full, 6)
coef(regfit.fwd, 6)
coef(regfit.bwd, 8)
n <- nrow(Hitters)
mod.F <- lm(Salary~., data=Hitters)
mod.bwd.bic <- step(mod.F, direction="backward", k=log(n), trace=1, steps=1000)
summary(mod.bwd.bic)
# compare with backward obtained above
coef(regfit.bwd, 8)
mod.F <- lm(Salary~., data=Hitters)
summary(mod.F)
mod.R <- update(mod.F, .~.-CHmRun)
anova(mod.R, mod.F)
summary(mod.R)
mod.R <- update(mod.R, .~.-Years)
anova(mod.R, mod.F)
summary(mod.R)
mod.R <- update(mod.R, .~.-NewLeague)
anova(mod.R, mod.F)
summary(mod.R)
mod.R <- update(mod.R, .~.-RBI)
anova(mod.R, mod.F)
summary(mod.R)
mod.R <- update(mod.R, .~.-CHits)
anova(mod.R, mod.F)
summary(mod.R)
mod.R <- update(mod.R, .~.-HmRun)
anova(mod.R, mod.F)
summary(mod.R)
mod.R <- update(mod.R, .~.-Errors)
anova(mod.R, mod.F)
summary(mod.R)
mod.R <- update(mod.R, .~.-Runs)
anova(mod.R, mod.F)
summary(mod.R)
mod.R <- update(mod.R, .~.-League)
anova(mod.R, mod.F)
summary(mod.R)
mod.R <- update(mod.R, .~.-Assists)
anova(mod.R, mod.F)
summary(mod.R)
mod.R <- update(mod.R, .~.-CAtBat)
anova(mod.R, mod.F)
summary(mod.R)
mod.RR <- update(mod.R, .~.-CRBI)
anova(mod.RR, mod.F)
summary(mod.RR)
# load and clean data
library(ISLR2)
Hitters <- na.omit(Hitters)
# fit ls model
#
lm.mod <- lm(Salary~., data=Hitters)
summary(lm.mod)
#ls = least squares
beta.hat.ls <- coefficients(lm.mod)[-1]
# L1 norm
ell_1.ls <- sum(abs(beta.hat.ls))
ell_1.ls
# L2 norm
ell_2.ls <- sqrt(sum(beta.hat.ls^2))
ell_2.ls
# package for ridge regression
library(glmnet)
grid <- 10^seq(10, -2, length=100)
plot(1:100, grid,  type="b", ylab = "lambda values", pch=20)
# look more closely to the smallest lambda values
plot(80:100, grid[80:100], type="b", ylab = "lambda values", pch=20)
# the grid is linear in the logarithm
plot(log10(grid), type="b", ylab="log-lambda values", pch=20)
summary(log10(grid))
X <- model.matrix(Salary~., data=Hitters)
X <- X[,-1] #removing the intercept
y <- Hitters$Salary
ridge.mod <- glmnet(X, y, alpha=0, lambda=grid)
# plot method for glmnet, produces
# a coefficient profile plot.
#
#  xvar = what is on the X-axis
#
#  xvar = "norm"   plots against the L1-norm
#  xvar = "lambda" against the log-lambda sequence
#  xvar = "dev" against the percent deviance explained
#
#glmnet object is already setup to work with the plot function
plot(ridge.mod, xvar="lambda")
# add labels to identify the variables
plot(ridge.mod, xvar="lambda", label=TRUE)
plot(ridge.mod, xvar = "norm", label=TRUE)
plot(ridge.mod, xvar = "dev",  label=TRUE)
dim(coef(ridge.mod)) #100 models, 20 coefficient for every model
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
# L2 norm and amount of shrinkage
sqrt(sum(coef(ridge.mod)[-1,50]^2))
sqrt(sum(coef(ridge.mod)[-1,50]^2))/ell_2.ls #a lot of shrinkage
plot(ridge.mod, xvar="lambda", label=TRUE)
abline(v=log(ridge.mod$lambda[50]), lty=3, lwd=2)
ridge.mod$lambda[70]
coef(ridge.mod)[,70]
# # L_2 norm and amount of shrinkage
sqrt(sum(coef(ridge.mod)[-1,70]^2))
sqrt(sum(coef(ridge.mod)[-1,70]^2))/ell_2.ls
abline(v=log(ridge.mod$lambda[70]), lty=3, lwd=2, col="red")
ridge.mod$lambda[50]
ridge.mod$lambda[49]
coef(ridge.mod, s=15000)
predict(ridge.mod, s=15000, type="coefficients")
y.hat <- predict(ridge.mod, s=15000, newx=X, type="response")
# select n/2 observations for training set
n <- nrow(X)
n/2
set.seed(1)
train <- sample(1:n, 131)
test  <- setdiff(1:n, train)
# fit ridge regression on the training set
ridge.mod <- glmnet(X[train, ], y[train], alpha = 0, lambda = grid)
plot(ridge.mod, xvar="lambda", label=TRUE)
abline(v=log(4), lty=3)
ridge.pred <- predict(ridge.mod, s = 4, newx = X[test, ], type="response")
y.test <- y[test]
mean((ridge.pred - y.test)^2)
abline(v=log(10^10), lty=3)
ridge.pred <- predict(ridge.mod, s = 1e10, newx = X[test, ])
mean((ridge.pred - y.test)^2)
round(coef(ridge.mod, s=1e10), 4)
# estimate the test MSE with lambda=0 so as to obtain the
# usual least square regression model
# notice that we use the argument exact=TRUE
# so the model is refitted and
# x = X[train, ], y = y[train]
# need to be supplied again
#
#with lambda = 0 we need to pass the training set because it can't interpolate lambda=0.
ridge.pred <- predict(ridge.mod, s = 0, newx = X[test, ],
exact = TRUE, x = X[train, ], y = y[train])
mean((ridge.pred - y.test)^2)
lm(Salary ~., data=Hitters, subset = train)
predict(ridge.mod, s = 0, exact = TRUE, type = "coefficients",
x = X[train, ], y = y[train])[1:20, ]
set.seed(1)
cv.out <- cv.glmnet(X[train, ], y[train], alpha = 0, nfold=10)
# lambda values
length(cv.out$lambda)
cv.out$lambda[1:10]
summary(cv.out$lambda)
cv.out$cvm[1:10]
cv.out$cvsd[1:10]
plot(cv.out)
# we have
lambda.50 <- cv.out$lambda[50]
mse.50 <-  cv.out$cvm[50]
mse.sd.50 <- cv.out$cvsd[50]
# mean squared error
points(log(lambda.50), mse.50, pch=16)
# mse +/- one sd
points(log(lambda.50), mse.50+mse.sd.50, pch=16, col="blue")
points(log(lambda.50), mse.50-mse.sd.50, pch=16, col="green")
i.bestlam <- which.min(cv.out$cvm)
i.bestlam
bestlam <- cv.out$lambda[i.bestlam]
bestlam
abline(v=log(bestlam), lwd=2, lty=3, col="blue")
cv.out$cvm[i.bestlam]
bestlam <- cv.out$lambda.min
bestlam
# identify the lambda.1se
bestlam.1se <- cv.out$lambda.1se
bestlam.1se
abline(h=cv.out$cvm[i.bestlam]+cv.out$cvsd[i.bestlam], lwd=2)
bestlam <- cv.out$lambda.min
ridge.pred <- predict(ridge.mod, s=bestlam, newx=X[test,])
test.mse.ridge <- mean((ridge.pred-y[test])^2)
test.mse.ridge
test.mse.ridge
test.mse.ls
ridge.final <- glmnet(X, y, alpha = 0)
coef(ridge.final, s=bestlam)
# function to be minimized
f <- function(beta, lambda, beta.hat){
y <- 0.5+0.5*beta^2-beta.hat*beta+lambda*abs(beta)
return(y)
}
# soft thresholding function
soft.thresh <- function(beta.hat, lambda) sign(beta.hat)*max(abs(beta.hat)-lambda, 0)
lambda <- 5
# case 1 beta.hat > lambda
#
beta.hat <- 8
# case 2 beta.hat < -lambda
#
beta.hat <- - 8
beta.hat <- 4
beta.hat <- -4
# see the different results of cases 1 to 3 above
beta.hat.L <- soft.thresh(beta.hat, lambda)
beta.hat.L
curve(f(x, lambda, beta.hat), -10, 10, ylab=expression(f[lambda](beta)), xlab=expression(beta))
abline(v=beta.hat.L, lty=3, col="red")
axis(1, at=beta.hat.L)
grid <- 10^seq(3, -2, length=100)
# alpha=0 is ridge regression
# alpha=1 is lasso regression
# thresh: convergence threshold for coordinate descent.
#
lasso.mod <- glmnet(X,y, alpha=1, lambda=grid, thresh= 1e-10)
plot(lasso.mod, xvar="lambda", label=TRUE)
plot(lasso.mod, xvar="norm", label=TRUE)
plot(lasso.mod, xvar="dev", label=TRUE)
# coefficients for different lambda values
##############################################
plot(lasso.mod, xvar="lambda", label=TRUE)
i <- 90 # try values 10, 30, 50, 70, 90
lasso.mod$lambda[i]
abline(v=log(lasso.mod$lambda[i]), lty=3)
beta.L <- round(coef(lasso.mod)[,i], 4)
beta.L <- beta.L[-1] # remove the intercept
# number of non-zero coefficients
sum(beta.L!=0)
# L1 norm
sum(abs(beta.L))
set.seed(10)
cv.out.lasso <- cv.glmnet(X[train,], y[train], alpha=1, nfold=10)
plot(cv.out.lasso)
bestlam <- cv.out.lasso$lambda.min
lasso.pred <- predict(lasso.mod, s=bestlam, newx=X[test,])
test.mse.lasso <- mean((lasso.pred-y[test])^2)
test.mse.lasso
test.mse.lasso
test.mse.ridge
test.mse.ls
Q
data("UCBAdmissions")
help(UCBAdmissions)
dim(UCBAdmissions)
dimnames(UCBAdmissions)
UCBAdmissions
tAGD <- UCBAdmissions
tAGD[,,1]
tAGD[,,"A"]
tAGD[2,,]
tAGD["Rejected",,]
tAGD[, 1,]
tAGD[, "Male",]
# is Admission independent of Gender?
#
margin.table(tAGD, c(1,2))
tAG <- margin.table(tAGD, c(1,2))
tAG
chisq.test(tAG)
tG <- margin.table(tAGD, 2)
tG
tAG
# proportions of admitted
tAG["Admitted",]
tAG["Admitted",]/tG
# proportions of rejected
tAG["Rejected",]
tAG["Rejected",]/tG
tAD <- margin.table(tAGD, c(1, 3))
tD <- margin.table(tAGD, 3)
tAD["Admitted",]/tD
# "MATHMARKS" DATA
library(gRbase)
data("mathmark")
names(mathmark)
mathmark[1:3,]
pairs(mathmark)
S <- cov(mathmark)
P <- cor(mathmark)
round(S, 1)
round(P, 2)
D <- diag(S)
D
D <- diag(D)
D
P <- solve(sqrt(D))%*%S%*%solve(sqrt(D))
round(P, 2)
P <- cov2cor(S)
mod1 <- lm(statistics ~. - analysis, data=mathmark)
mod2 <- lm(analysis ~. -statistics , data=mathmark)
cor(residuals(mod1), residuals(mod2))
round(solve(S), 6)
-cov2cor(solve(S))
library(igraph)
data(mathmark)
S <- var(mathmark)
R <- -cov2cor(solve(S))
thr <- 0.1
G <- abs(R)>thr
diag(G) <- 0
G
# notice that rows and columns have names
rownames(G)
colnames(G)
dimnames(G)
Gi <- as(G, "igraph")
plot(Gi)
setwd("C:/Users/gabri/Desktop/code/Statistical-Learning")
# Upload datasets
data_salary <- read_excel("NBA_players_salaries_HH.xlsx")
# Upload packages
library(readxl)
# Upload datasets
data_salary <- read_excel("NBA_players_salaries_HH.xlsx")
