test  <- setdiff(1:n, train)
# fit ridge regression on the training set
ridge.mod <- glmnet(X[train, ], y[train], alpha = 0, lambda = grid)
plot(ridge.mod, xvar="lambda", label=TRUE)
abline(v=log(4), lty=3)
ridge.pred <- predict(ridge.mod, s = 4, newx = X[test, ], type="response")
mean((ridge.pred - y[test])^2)
abline(v=log(10^10), lty=3)
ridge.pred <- predict(ridge.mod, s = 1e10, newx = X[test, ])
mean((ridge.pred - y[test])^2)
round(coef(ridge.mod, s=1e10), 4)
ridge.pred <- predict(ridge.mod, s = 0, newx = X[test, ],
exact = TRUE, x = X[train, ], y = y[train])
test.mse.ls <- mean((ridge.pred - y[test])^2)
test.mse.ls
library(glmnet)
set.seed(1)
cv.out <- cv.glmnet(X[train, ], y[train], alpha = 0, nfold=10)
# lambda values
length(cv.out$lambda)
cv.out$lambda[1:10]
summary(cv.out$lambda)
cv.out$cvm[1:10]
cv.out$cvsd[1:10]
plot(cv.out)
# we have
lambda.50 <- cv.out$lambda[50]
mse.50 <-  cv.out$cvm[50]
mse.sd.50 <- cv.out$cvsd[50]
# mean squared error
points(log(lambda.50), mse.50, pch=16)
# mse +/- one sd
points(log(lambda.50), mse.50+mse.sd.50, pch=16, col="blue")
points(log(lambda.50), mse.50-mse.sd.50, pch=16, col="green")
i.bestlam <- which.min(cv.out$cvm)
i.bestlam
bestlam <- cv.out$lambda[i.bestlam]
bestlam
abline(v=log(bestlam), lwd=2, lty=3, col="blue")
cv.out$cvm[i.bestlam]
min(cv.out$cvm)
bestlam <- cv.out$lambda.min
bestlam
bestlam <- cv.out$lambda.min
ridge.mod <- glmnet(X[train, ], y[train], alpha=0)
ridge.pred <- predict(ridge.mod, s=bestlam, newx=X[test,])
test.mse.ridge <- mean((ridge.pred-y[test])^2)
test.mse.ridge
test.mse.ridge
test.mse.ls
ridge.final <- glmnet(X, y, alpha = 0)
coef(ridge.final, s=bestlam)
ridge.final <- glmnet(X, y, alpha = 0)
coef(ridge.final, s=bestlam)
plot(ridge.final, xvar="lambda", label=TRUE)
abline(v=log(bestlam), lty=3, lwd=2)
X <- model.matrix(Salary~., data=fd_numeric)
X <- X[,-1]
y <- fd_numeric$Salary
# select n/2 observations for training set
n <- nrow(X)
n/2
cv.out <- cv.glmnet(X[train, ], y[train], alpha = 0, nfold=10)
plot(cv.out)
X <- model.matrix(Salary~., data=fd_numeric)
X <- X[,-1]
y <- fd_numeric$Salary
# package for ridge regression
library(glmnet)
# select n/2 observations for training set
n <- nrow(X)
n/2
set.seed(1)
train <- sample(1:n, 180)
test  <- setdiff(1:n, train)
cv.out <- cv.glmnet(X[train, ], y[train], alpha = 0, nfold=10)
plot(cv.out)
best_lambda <- cv_model$lambda.min
best_lambda <- cv.out$lambda.min
best_lambda
min(cv.out$cvm)
i.bestlam <- which.min(cv.out$cvm)
i.bestlam
bestlam <- cv.out$lambda[i.bestlam]
bestlam
abline(v=log(bestlam), lwd=2, lty=3, col="blue")
cv.out$cvm[i.bestlam]
min(cv.out$cvm)
best_lambda <- cv.out$lambda.min
best_lambda
min(cv.out$cvm)
plot(cv.out)
best_lambda <- cv.out$lambda.min
best_lambda
min(cv.out$cvm)
ridge.mod <- glmnet(X[train, ], y[train], alpha=0)
ridge.pred <- predict(ridge.mod, s=bestlam, newx=X[test,])
test.mse.ridge <- mean((ridge.pred-y[test])^2)
test.mse.ridge
test.mse.ridge
ridge.final <- glmnet(X, y, alpha = 0)
coef(ridge.final, s=bestlam)
plot(ridge.final, xvar="lambda", label=TRUE)
abline(v=log(bestlam), lty=3, lwd=2)
test.mse.ridge <- mean((ridge.pred-y)^2)
coef(ridge.final, s=bestlam)
# final MSE
ridge.final.pred <- predict(ridge.final, s=bestlam, X)
test.mse.ridge <- mean((ridge.final.pred-y)^2)
test.mse.ridge
ridge.final.pred
len(ridge.final.pred)
length(ridge.final.pred)
diffs <- ridge.final.pred - fd_numeric$Salary
diffs
sort(diffs)
diffs <- ridge.final.pred - fd_numeric$Salary
diffs
diffs$names <- rownames(diffs)
rownames(diffs)
indexes(diffs)
index(diffs)
diffs
diffs <- ridge.final.pred - fd_numeric$Salary
diffs
diffs$names
diffs_transformed <- diffs %>%
rownames_to_column(var = "Index")
library(dplyr)
diffs_transformed <- diffs %>%
rownames_to_column(var = "Index")
library(dplyr)
diffs_transformed <- diffs %>%
rownames_to_column(var = "Index")
names(diffs)
diffs
diffs[3]
sorted_diffs <- sort(diffs)
sorted_diffs
sorted_diffs <- sort(abs(diffs))
sorted_diffs
diffs[diffs = [16337434.87]
diffs[diffs = 16337434.87]
diffs[diffs == 16337434.87]
diffs == 16337434.87
diffs == 16337434.87 == TRUE
diffs == abs(16337434.87)
diffs == -16337434.87
diffs <- ridge.final.pred - fd_numeric$Salary
as.data.frame(diffs)
View(diffs)
as.data.frame(abs(diffs))
View(diffs)
as.data.frame(abs(diffs))
View(diffs)
as.data.frame(abs(diffs))
diffs
diffs
view(diffs)
View(diffs)
View(diffs)
as.data.frame(diffs)
sort(diffs)
sort(abs(diffs))
diffs[s1 == 16497734.18]
View(diffs)
diffs[diffs$s1 == 16497734.18]
diffs <- as.data.frame(diffs)
diffs[diffs$s1 == 16497734.18]
diffs
sort(abs(diffs))
diffs
sort(diffs$s1)
diffs[diffs$s1 == 16497734.18]
diffs[diffs$s1 = 16497734.18]
diffs$s1 == 16497734.18
diffs$s1 == -16497734.18
View(diffs)
# Upload packages
library(readxl)
# Upload datasets
data_salary <- read_excel("NBA_players_salaries_HH.xlsx")
data_traditional_per48 <- read.csv("RS_traditional_per48.csv")
data_traditional_tot <- read.csv("RS_traditional_TOTALS.csv")
data_advanced <- read.csv("RS_advanced_per48.csv")
data_miscellaneous <- read.csv("RS_miscellaneous_per48.csv")
data_vorp <- read_excel("vorp.xlsx")
# creating a new column in data_traditional_tot: MIN_G (minutes played per game)
# min/gp
MIN_G <- data_traditional_tot$MIN/data_traditional_tot$GP
data_traditional_tot <- cbind(data_traditional_tot, MIN_G)
# Select the features (columns) of interest
data_salary <- data_salary[, c(2, 3)]
data_traditional_per48 <- data_traditional_per48[, c(3, 7, 8, 15, 18, 21:32)]
data_traditional_tot <- data_traditional_tot[, c(3, 12, 68)]
data_advanced <- data_advanced[, c(3, 14, 17, 20, 23, 31, 32, 38)]
data_miscellaneous <- data_miscellaneous[, c(3, 24)]
data_vorp <- data_vorp[, c(2, 3, 23, 31, 32)]
# Rename a column to prepare the dataset to do a merge
names(data_salary)[names(data_salary) == "Player"] <- "PLAYER_NAME"
data_trad_tot <- data_traditional_tot[data_traditional_tot$MIN > 480, ]  # considering players with at least 480 minutes played along the season (better measure compared to games played)
# Merge the datasets applying a full join
data_st <- merge(data_salary, data_traditional_per48, by = "PLAYER_NAME", all = TRUE)
data_ast <- merge(data_st, data_advanced, by = "PLAYER_NAME", all = TRUE)
data_mast <- merge(data_ast, data_miscellaneous, by = "PLAYER_NAME", all = TRUE)
data_mastt <- merge(data_mast, data_trad_tot, by = "PLAYER_NAME", all = TRUE)
final_dataset <- merge(data_mastt, data_vorp, by = "PLAYER_NAME", all = TRUE)
# Facing NA values problem
# data_mast <- data_mast[!is.na(data_mast$2023/24), ]  ## not necessary, names correspond now
final_dataset <- final_dataset[!is.na(final_dataset$AGE), ]     ## NA's we need to remove because no stats on NBA.com
final_dataset <- final_dataset[!is.na(final_dataset$MIN), ]
final_dataset <- final_dataset[!is.na(final_dataset$VORP), ] ## removing NA's for MIN (players with less than 480 minutes played)
final_dataset
# Ensure that there aren't NA values
NA_number <- colSums(is.na(final_dataset))
print(NA_number)
final_dataset <- final_dataset[, -17]
colnames(final_dataset)[colnames(final_dataset) == 'PFD.y'] <- 'PFD'
colnames(final_dataset)[colnames(final_dataset) == '2023/24'] <- 'Salary'
final_dataset$Salary <- as.numeric(gsub("[\\$\\,]", "", final_dataset$Salary))
class(final_dataset$Salary)
attach(final_dataset)
numeric_cols <- sapply(final_dataset, is.numeric)
fd_numeric <- final_dataset[, numeric_cols]
summary(fd_numeric)
boxplot(Salary)
summary(Salary)
hist(Salary)
boxplot(AGE)
boxplot(GP)
boxplot(MIN)
boxplot(MIN_G, PTS)
boxplot(OREB, DREB, REB, AST)
boxplot(TOV, STL, BLK, BLKA, PF, PFD)
boxplot(FG_PCT, FG3_PCT, FT_PCT, TS_PCT)
boxplot(OFF_RATING, DEF_RATING)
boxplot(NET_RATING)
boxplot(AST_TO)
boxplot(PIE, USG_PCT)
boxplot(WS, BPM, VORP)
cov_mat <- round(cov(fd_numeric),2)
cor_mat <- round(cor(fd_numeric),2)
library(corrplot)
corrplot(cor(fd_numeric), method = 'color')
corrplot(cor(fd_numeric), method = 'ellipse')
panel.hist <- function(x, ...)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(usr[1:2], 0, 1.5) )
h <- hist(x, plot = FALSE)
breaks <- h$breaks; nB <- length(breaks)
y <- h$counts; y <- y/max(y)
rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r <- abs(cor(x, y))
txt <- format(c(r, 0.123456789), digits = digits)[1]
txt <- paste0(prefix, txt)
if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(fd_numeric, diag.panel=panel.hist, upper.panel=panel.cor)
lm.mod <- lm(Salary~+., data=fd_numeric)
summary(lm.mod)
par(mfrow = c(2, 2))
plot(lm.mod)
lm.log <- lm(log(Salary)~+., data=fd_numeric)
summary(lm.log)
plot(lm.log)
lm.step <- step(lm.mod)
summary(lm.step)  # model with the lowest AIC, interpretation
lm.step$coefficients
cor(fd_numeric[c("AGE", "FG_PCT", "DREB", "TOV", "BLKA", "PF",
"PTS", "OFF_RATING", "DEF_RATING", "NET_RATING", "TS_PCT", "PIE",
"MIN", "MIN_G", "WS")])
lm.step.coeff <- fd_numeric[c("AGE", "FG_PCT", "DREB", "TOV", "BLKA", "PF",
"PTS", "OFF_RATING", "DEF_RATING", "NET_RATING", "TS_PCT", "PIE",
"MIN", "MIN_G", "WS")]  # interpretation
par(mfrow = c(1,1))
corrplot(cor(lm.step.coeff), method = 'number')  # we can observe strong correlations (commentare) so...go with ridge
# Define the number of folds for cross-validation
num_folds <- 10
# Set the seed for reproducibility
set.seed(123)
# Initialize vectors to store performance metrics
mse <- numeric(num_folds)
rsquared <- numeric(num_folds)
# Perform k-fold cross-validation
for (i in 1:num_folds) {
# Create indices for train and test sets
test_indices <- ((i - 1) * nrow(fd_numeric) / num_folds + 1):(i * nrow(fd_numeric) / num_folds)
train_data <- fd_numeric[-test_indices, ]
test_data <- fd_numeric[test_indices, ]
# Fit the linear regression model on the training data
model <- lm(Salary ~ AGE + FG_PCT + DREB + TOV + BLKA + PF +
PTS + OFF_RATING + DEF_RATING + NET_RATING + TS_PCT + PIE +
MIN + MIN, data = train_data)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data)
# Calculate performance metrics
mse[i] <- mean((test_data$Salary - predictions)^2)
rsquared[i] <- summary(model)$r.squared
}
# Calculate average performance metrics
avg_mse <- mean(mse)
avg_rsquared <- mean(rsquared)
# Print average performance metrics
cat("Average MSE:", avg_mse, "\n")
cat("Average R-squared:", avg_rsquared, "\n")
lm.step.log <- step(lm.log)
summary(lm.step.log)
lm.step.log$coefficients
cor(fd_numeric[c("AGE", "FG_PCT", "BLK", "BLKA", "PTS",
"OFF_RATING", "DEF_RATING", "NET_RATING", "TS_PCT", "PIE", "MIN_G",
"WS")])
# Define the number of folds for cross-validation
num_folds <- 10
# Set the seed for reproducibility
set.seed(123)
# Initialize vectors to store performance metrics
mse_log <- numeric(num_folds)
rsquared_log <- numeric(num_folds)
# Perform k-fold cross-validation
for (i in 1:num_folds) {
# Create indices for train and test sets
test_indices <- ((i - 1) * nrow(fd_numeric) / num_folds + 1):(i * nrow(fd_numeric) / num_folds)
train_data <- fd_numeric[-test_indices, ]
test_data <- fd_numeric[test_indices, ]
# Fit the linear regression model on the training data
model <- lm(log(Salary) ~ AGE + FG_PCT + BLK + BLKA + PTS +
OFF_RATING + DEF_RATING + NET_RATING + TS_PCT + PIE + MIN_G +
WS, data = train_data)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data)
# Calculate performance metrics
mse_log[i] <- mean((test_data$Salary - 2.71^predictions)^2)
rsquared_log[i] <- summary(model)$r.squared
}
# Calculate average performance metrics
avg_mse_log <- mean(mse_log)
avg_rsquared_log <- mean(rsquared_log)
# Print average performance metrics
cat("Average MSE:", avg_mse_log, "\n")
cat("Average R-squared:", avg_rsquared_log, "\n")
pred_values <- predict(lm.step)
plot(pred_values, Salary)
res <- lm.step$residuals
largest_residuals_indices <- order(abs(res), decreasing=TRUE)
salaries <- fd_numeric[c(32,191,310,349,96,115,184,78,158,334),c("Salary","MIN_G")]
pred_values1 <- pred_values[c(32,191,310,349,96,115,184,78,158,334)]
res_values <- res[c(32,191,310,349,96,115,184,78,158,334)]
matching <- final_dataset[final_dataset$Salary %in% salaries$Salary & final_dataset$MIN_G %in% salaries$MIN_G, ]
matching_sort <- matching[order(matching$Salary),]
sal_pred_res <- cbind(salaries$Salary,pred_values1,res_values)
sal_pred_res_sort <- sal_pred_res[order(sal_pred_res[,1]),]
tab1 <- cbind(matching_sort$PLAYER_NAME,sal_pred_res_sort)
lm.mod <- lm(Salary~+., data=fd_numeric)
summary(lm.mod)
X <- model.matrix(Salary~., data=fd_numeric)
X <- X[,-1]
y <- fd_numeric$Salary
# package for ridge regression
library(glmnet)
# select n/2 observations for training set
n <- nrow(X)
n/2
set.seed(1)
train <- sample(1:n, 180)
test  <- setdiff(1:n, train)
cv.out <- cv.glmnet(X[train, ], y[train], alpha = 0, nfold=10)
plot(cv.out)
best_lambda <- cv.out$lambda.min
best_lambda
ridge.mod <- glmnet(X[train, ], y[train], alpha=0)
ridge.pred <- predict(ridge.mod, s=bestlam, newx=X[test,])
test.mse.ridge <- mean((ridge.pred-y[test])^2)
test.mse.ridge
ridge.final <- glmnet(X, y, alpha = 0)
coef(ridge.final, s=bestlam)
plot(ridge.final, xvar="lambda", label=TRUE)
abline(v=log(bestlam), lty=3, lwd=2)
# final MSE
ridge.final.pred <- predict(ridge.final, s=bestlam, X)
test.mse.ridge <- mean((ridge.final.pred-y)^2)
test.mse.ridge
diffs <- ridge.final.pred - fd_numeric$Salary
type(diffs)
class(diffs)
diffs
order(abs(diffs))
diffs
order(abs(diffs))
diffs[32]
ord_indexes <- order(abs(diffs), decreasing = TRUE)
ord_indexes
ord_indexes[32]
ord_indexes[1:10]
big_diffs_ind <- ord_indexes[1:10]
ridge.final.pred[big_diffs_ind]
ridge.final.pred[349]
ridge.final.pred
ridge.final.pred[big_diffs_ind]
View(final_dataset)
final_dataset[big_diffs_ind]
final_dataset[big_diffs_ind, :]
final_dataset[:,big_diffs_ind]
final_dataset[c(big_diffs_ind)]
final_dataset[c(big_diffs_ind), :]
final_dataset[c(big_diffs_ind), ]
final_dataset[c(big_diffs_ind), ][c(final_dataset$PLAYER_NAME, final_dataset$Salary)]
fd_ridge[c('PLAYER_NAME', 'Salary')]
fd_ridge <- final_dataset[c(big_diffs_ind), ]
fd_ridge[c('PLAYER_NAME', 'Salary')]
pred_sal_ridge <- ridge.final.pred[big_diffs_ind]
tab2 <- cbind(fd_ridge, pred_sal_ridge, fd_ridge$Salary-pred_sal_ridge)
tab2
fd_ridge_cut <- fd_ridge[c('PLAYER_NAME', 'Salary')]
tab2 <- cbind(fd_ridge_cut, pred_sal_ridge, fd_ridge_cut$Salary-pred_sal_ridge)
tab2
tab1 <- cbind(matching_sort$PLAYER_NAME,sal_pred_res_sort)
lm.mod <- lm(Salary~+., data=fd_numeric)
tab1
tab2
# final MSE
ridge.final.pred <- predict(ridge.final, s=bestlam, X)
#use fitted best model to make predictions
y_predicted <- predict(model, s = best_lambda, newx = X)
#use fitted best model to make predictions
y_predicted <- predict(ridge.final)
#use fitted best model to make predictions
y_predicted <- predict(ridge.final, s = bestlam, newx = X)
#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)
#find R-Squared
rsq <- 1 - sse/sst
rsq
#find R-Squared
R2 <- 1 - sse/sst
R2
rsquared_log <- numeric(num_folds)
summary(lm.step.log)
# Calculate average performance metrics
avg_mse <- mean(mse)
avg_rsquared <- mean(rsquared)
avg_rsquared
data_traditional_per48
data_salary
data_vorp
pos <- data_vorp$PLAYER_NAME, data_vorp$Pos
pos <- cbind(data_vorp$PLAYER_NAME, data_vorp$Pos)
pos
ds_wpos <- merge(final_dataset, pos, by = "PLAYER_NAME", all = TRUE)
final_dataset
pos
pos <- as.Data.Frame(cbind(data_vorp$PLAYER_NAME, data_vorp$Pos))
pos <- as.data.frame(cbind(data_vorp$PLAYER_NAME, data_vorp$Pos))
pos
names(pos) <- c("PLAYER_NAME", "POS")
pos
ds_wpos <- merge(final_dataset, pos, by = "PLAYER_NAME", all = TRUE)
ds_wpos
# Upload packages
library(readxl)
# Upload datasets
data_salary <- read_excel("NBA_players_salaries_HH.xlsx")
data_traditional_per48 <- read.csv("RS_traditional_per48.csv")
data_traditional_tot <- read.csv("RS_traditional_TOTALS.csv")
data_advanced <- read.csv("RS_advanced_per48.csv")
data_miscellaneous <- read.csv("RS_miscellaneous_per48.csv")
data_vorp <- read_excel("vorp.xlsx")
# creating a new column in data_traditional_tot: MIN_G (minutes played per game)
# min/gp
MIN_G <- data_traditional_tot$MIN/data_traditional_tot$GP
data_traditional_tot <- cbind(data_traditional_tot, MIN_G)
# Select the features (columns) of interest
data_salary <- data_salary[, c(2, 3)]
data_traditional_per48 <- data_traditional_per48[, c(3, 7, 8, 15, 18, 21:32)]
data_traditional_tot <- data_traditional_tot[, c(3, 12, 68)]
data_advanced <- data_advanced[, c(3, 14, 17, 20, 23, 31, 32, 38)]
data_miscellaneous <- data_miscellaneous[, c(3, 24)]
data_vorp <- data_vorp[, c(2, 3, 23, 31, 32)]
# Rename a column to prepare the dataset to do a merge
names(data_salary)[names(data_salary) == "Player"] <- "PLAYER_NAME"
data_trad_tot <- data_traditional_tot[data_traditional_tot$MIN > 480, ]  # considering players with at least 480 minutes played along the season (better measure compared to games played)
# Merge the datasets applying a full join
data_st <- merge(data_salary, data_traditional_per48, by = "PLAYER_NAME", all = TRUE)
data_ast <- merge(data_st, data_advanced, by = "PLAYER_NAME", all = TRUE)
data_mast <- merge(data_ast, data_miscellaneous, by = "PLAYER_NAME", all = TRUE)
data_mastt <- merge(data_mast, data_trad_tot, by = "PLAYER_NAME", all = TRUE)
final_dataset <- merge(data_mastt, data_vorp, by = "PLAYER_NAME", all = TRUE)
# Facing NA values problem
# data_mast <- data_mast[!is.na(data_mast$2023/24), ]  ## not necessary, names correspond now
final_dataset <- final_dataset[!is.na(final_dataset$AGE), ]     ## NA's we need to remove because no stats on NBA.com
final_dataset <- final_dataset[!is.na(final_dataset$MIN), ]
final_dataset <- final_dataset[!is.na(final_dataset$VORP), ] ## removing NA's for MIN (players with less than 480 minutes played)
final_dataset
# Ensure that there aren't NA values
NA_number <- colSums(is.na(final_dataset))
print(NA_number)
final_dataset <- final_dataset[, -17]
colnames(final_dataset)[colnames(final_dataset) == 'PFD.y'] <- 'PFD'
colnames(final_dataset)[colnames(final_dataset) == '2023/24'] <- 'Salary'
final_dataset$Salary <- as.numeric(gsub("[\\$\\,]", "", final_dataset$Salary))
class(final_dataset$Salary)
ds_wpos <- merge(final_dataset, pos, by = "PLAYER_NAME", all = TRUE)
ds_wpos
ds_wpos <- merge(final_dataset, pos, by = "PLAYER_NAME")
ds_wpos
